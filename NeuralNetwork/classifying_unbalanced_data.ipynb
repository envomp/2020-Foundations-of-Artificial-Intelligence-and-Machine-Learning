{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "05 - tasakaalustamata andmetega klassifitseerimine - ÃœLESANNE",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/envomp/2020-Foundations-of-Artificial-Intelligence-and-Machine-Learning/blob/master/NeuralNetwork/classifying_unbalanced_data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "13IL9UJnKtno"
      },
      "source": [
        "# Unbalanced classification example\n",
        "\n",
        "Loads CSV files from the python list so that each file represents a separate class. Then tries to find the best network configuration and hyperparameters for the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bvFOy9S8KkK4"
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "!pip install optuna\n",
        "import optuna\n",
        "from torch.utils.data import TensorDataset, ConcatDataset, DataLoader\n",
        "import math\n",
        "import copy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ofIR8RrkuPQ1"
      },
      "source": [
        "# download and see what is inside\n",
        "!wget http://linuxator.com/data/mlaine/data_class1_s.csv\n",
        "!wget http://linuxator.com/data/mlaine/data_class2_d.csv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G3wSQ-9e6RWU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "858db692-b88a-4a82-cc21-5f12df8b6a0c"
      },
      "source": [
        "# this is to allow computation on GPU. To use this, enable under Runtime -> Change Runtime type\n",
        "# GPU should be already enabled on most cases when using this sheet as a templete\n",
        "# NB! Be sure to Restart runtime after this or you may get some very odd errors!\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(\"Using\", device)"
      ],
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ym59nUirb0e"
      },
      "source": [
        "# this method accepts the list of CSV files\n",
        "# first class (index 0) should be underrepresented class\n",
        "# it will be oversampled 50 times\n",
        "def build_dataset(csv_list):\n",
        "  class_num = 0\n",
        "  train_datasets = []\n",
        "  val_datasets = []\n",
        "  weights = []\n",
        "  # each file contains a single class\n",
        "  for csv_file in csv_list:\n",
        "    df = pd.read_csv(csv_file)\n",
        "    x = torch.from_numpy(df.iloc[:,1:].values.astype(np.float64)).float()\n",
        "\n",
        "    # split train/val  80%/20% in each CSV file\n",
        "    train_cnt = math.floor(x.shape[0] * 0.8)\n",
        "    val_cnt = x.shape[0] - train_cnt\n",
        "\n",
        "    # oversample class 0 which is undersampled\n",
        "    if class_num == 0:\n",
        "      # split 80/20\n",
        "      train_x, val_x = torch.utils.data.random_split(x, [train_cnt, val_cnt])\n",
        "      # repeat class 0 50 times\n",
        "      #print('x', train_x.__len__(), 'val', val_x.__len__())\n",
        "      # we oversample after splitting to avoid having same example both in\n",
        "      # training and validation set\n",
        "      train_x = x[train_x.indices].repeat([50,1])\n",
        "      val_x = x[val_x.indices].repeat([50,1])\n",
        "      #print('x', train_x.__len__(), 'val', val_x.__len__())\n",
        "      # generate ground truth tensors\n",
        "      train_y = torch.empty((train_x.shape[0]), dtype=torch.long).fill_(class_num)\n",
        "      val_y = torch.empty((val_x.shape[0]), dtype=torch.long).fill_(class_num)\n",
        "      # compose datasets\n",
        "      train_dataset = TensorDataset(train_x, train_y)\n",
        "      val_dataset = TensorDataset(val_x, val_y)\n",
        "    # keep every other class as they are\n",
        "    else:\n",
        "      # ground truth generation, just use class number\n",
        "      y = torch.empty((x.shape[0]), dtype=torch.long).fill_(class_num)\n",
        "      dataset = TensorDataset(x, y)\n",
        "      # this is how you normally split a dataset\n",
        "      train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_cnt, val_cnt])\n",
        "    \n",
        "    train_datasets.append(train_dataset)\n",
        "    val_datasets.append(val_dataset)\n",
        "\n",
        "    # save num of rows to calculate weights for loss fn\n",
        "    weights.append(train_dataset.__len__())\n",
        "    class_num += 1\n",
        "  \n",
        "  num_classes = class_num\n",
        "  celoss_weights = 1.0 - (torch.tensor(weights).float() / sum(weights))\n",
        "  \n",
        "  return ConcatDataset(train_datasets), ConcatDataset(val_datasets), num_classes, celoss_weights"
      ],
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2K8Pdo_PIp1P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1de0b0bc-191b-4aa1-ef2c-910b261f091f"
      },
      "source": [
        "train_dataset, val_dataset, num_classes, celoss_weights = build_dataset(['data_class1_s.csv', 'data_class2_d.csv'])\n",
        "print(train_dataset.__len__(), val_dataset.__len__())"
      ],
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "55739 13935\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tdbpmg_VaeoA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "250e991e-8699-43dd-b532-7806fe0c526d"
      },
      "source": [
        "# celoss_weights[0] = celoss_weights[0] * 3\n",
        "print(celoss_weights)"
      ],
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([0.6089, 0.3911])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nBSSWGGRK7gw"
      },
      "source": [
        "class ClassificationNetwork(nn.Module):\n",
        "\n",
        "  def __init__(self, num_classes, trial):\n",
        "    super().__init__()\n",
        "    self.num_classes = num_classes\n",
        "    num_layers = trial.suggest_int(\"num_layers\", 1, 3)\n",
        "    in_features = 8\n",
        "    layers = []\n",
        "\n",
        "    for layer_num in range(num_layers):\n",
        "      out_features = trial.suggest_int(\"out_features_{}\".format(layer_num), 2, 10)\n",
        "      layers.append(nn.Linear(in_features=in_features, out_features=out_features))\n",
        "      layers.append(nn.ReLU())\n",
        "      in_features = out_features\n",
        "      \n",
        "    layers.append(nn.Linear(in_features=in_features, out_features=num_classes))\n",
        "    self.model = nn.Sequential(*layers)\n",
        "\n",
        "  # method that runs the model and returns the result\n",
        "  # it is always called forward()\n",
        "  def forward(self, data):\n",
        "    return self.model(data)\n"
      ],
      "execution_count": 113,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tfreiJOhNWTB"
      },
      "source": [
        "def objective(trial): \n",
        "  model = ClassificationNetwork(num_classes, trial).to(device)\n",
        "  \n",
        "  batch_size = 128\n",
        "  dl = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "  learning_rate = trial.suggest_discrete_uniform(\"learning_rate\", 0.001, 0.5, 0.002)\n",
        "  optimiser = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "  loss = nn.CrossEntropyLoss(celoss_weights).to(device)\n",
        "  \n",
        "  epochs = 5\n",
        "  for i in range (0, epochs):\n",
        "    model.train()\n",
        "\n",
        "    for x, y in dl:\n",
        "      x, y = x.to(device), y.to(device)\n",
        "      optimiser.zero_grad()\n",
        "      prediction = model(x)\n",
        "      iter_loss = loss(prediction, y)\n",
        "      iter_loss.backward()\n",
        "      optimiser.step()\n",
        "\n",
        "    trial.set_user_attr(\"model\", model)    \n",
        "    val_loss = validate(model)\n",
        "    trial.report(val_loss, i)\n",
        "    if trial.should_prune():\n",
        "      raise optuna.TrialPruned()\n",
        "\n",
        "  return val_loss\n"
      ],
      "execution_count": 119,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VvjZ_PkoKTGQ"
      },
      "source": [
        "# Let's write the validation code!\n",
        "\n",
        "1. Put model into evaluation mode\n",
        "2. Use model to predict on validation data\n",
        "3. Evaluate results using loss function\n",
        "\n",
        "Evaluation\n",
        "\n",
        "1. Does not update model parameters\n",
        "2. Hence no need to compute gradients"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iVtHJPNHj4W1"
      },
      "source": [
        "def validate(model, output=False):\n",
        "  model.eval()\n",
        "  total_loss = 0.0\n",
        "  loss = nn.CrossEntropyLoss(celoss_weights.to(device))\n",
        "\n",
        "  dl = DataLoader(val_dataset, batch_size=1)\n",
        "  ok = [0,0]\n",
        "  cls_cnt = [0,0]\n",
        "\n",
        "  with torch.no_grad(): # no gradient calculation inside this block\n",
        "    for data in dl:\n",
        "      x, y = data[0].to(device), data[1].to(device)\n",
        "      prediction = model(x)\n",
        "      \n",
        "      iter_loss = loss(prediction, y)\n",
        "\n",
        "      if output is True:\n",
        "        cls_cnt[y.item()] += 1\n",
        "        norm_pred = torch.nn.functional.softmax(prediction)\n",
        "        # print out to debug values\n",
        "        if norm_pred[0][y.item()] > 0.5:\n",
        "          ok[y.item()] += 1\n",
        "      total_loss += iter_loss.item()\n",
        "\n",
        "  total_loss = total_loss/val_dataset.__len__()\n",
        "  if output is True:\n",
        "    print(\"Mean loss:\", total_loss, \"total\", cls_cnt, \"correct\", ok)\n",
        "\n",
        "  return total_loss\n"
      ],
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WTqpkfL_3SKI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "611e8152-1a63-4c05-a3d0-e315401b2e05"
      },
      "source": [
        "study = optuna.create_study(direction=\"minimize\", pruner=optuna.pruners.MedianPruner(n_startup_trials=10, n_warmup_steps=10))\n",
        "study.optimize(objective, n_trials=100)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2021-04-04 13:49:27,044]\u001b[0m A new study created in memory with name: no-name-ef5fa6cd-8f2e-40a8-a20c-1c908e8f1a88\u001b[0m\n",
            "/usr/local/lib/python3.7/dist-packages/optuna/distributions.py:550: UserWarning:\n",
            "\n",
            "The distribution is specified by [0.001, 0.5] and q=0.002, but the range is not divisible by `q`. It will be replaced by [0.001, 0.499].\n",
            "\n",
            "\u001b[32m[I 2021-04-04 13:50:11,051]\u001b[0m Trial 0 finished with value: 0.6958707416831784 and parameters: {'num_layers': 2, 'out_features_0': 6, 'out_features_1': 2, 'learning_rate': 0.039}. Best is trial 0 with value: 0.6958707416831784.\u001b[0m\n",
            "\u001b[32m[I 2021-04-04 13:50:48,074]\u001b[0m Trial 1 finished with value: 0.6841793399923028 and parameters: {'num_layers': 1, 'out_features_0': 3, 'learning_rate': 0.329}. Best is trial 1 with value: 0.6841793399923028.\u001b[0m\n",
            "\u001b[32m[I 2021-04-04 13:51:38,784]\u001b[0m Trial 2 finished with value: 0.6833625862065463 and parameters: {'num_layers': 3, 'out_features_0': 4, 'out_features_1': 4, 'out_features_2': 6, 'learning_rate': 0.10300000000000001}. Best is trial 2 with value: 0.6833625862065463.\u001b[0m\n",
            "\u001b[32m[I 2021-04-04 13:52:22,163]\u001b[0m Trial 3 finished with value: 0.6900032640016759 and parameters: {'num_layers': 2, 'out_features_0': 4, 'out_features_1': 3, 'learning_rate': 0.355}. Best is trial 2 with value: 0.6833625862065463.\u001b[0m\n",
            "\u001b[32m[I 2021-04-04 13:52:59,237]\u001b[0m Trial 4 finished with value: 844.1651143632228 and parameters: {'num_layers': 1, 'out_features_0': 7, 'learning_rate': 0.003}. Best is trial 2 with value: 0.6833625862065463.\u001b[0m\n",
            "\u001b[32m[I 2021-04-04 13:53:36,479]\u001b[0m Trial 5 finished with value: 0.6871142571529352 and parameters: {'num_layers': 1, 'out_features_0': 6, 'learning_rate': 0.217}. Best is trial 2 with value: 0.6833625862065463.\u001b[0m\n",
            "\u001b[33m[W 2021-04-04 13:54:26,082]\u001b[0m Trial 6 failed, because the objective function returned nan.\u001b[0m\n",
            "\u001b[32m[I 2021-04-04 13:55:09,644]\u001b[0m Trial 7 finished with value: 0.6954226956001353 and parameters: {'num_layers': 2, 'out_features_0': 3, 'out_features_1': 6, 'learning_rate': 0.219}. Best is trial 2 with value: 0.6833625862065463.\u001b[0m\n",
            "\u001b[32m[I 2021-04-04 13:55:59,346]\u001b[0m Trial 8 finished with value: 0.6912948770423837 and parameters: {'num_layers': 3, 'out_features_0': 6, 'out_features_1': 7, 'out_features_2': 4, 'learning_rate': 0.455}. Best is trial 2 with value: 0.6833625862065463.\u001b[0m\n",
            "\u001b[32m[I 2021-04-04 13:56:42,712]\u001b[0m Trial 9 finished with value: 0.6902949594664326 and parameters: {'num_layers': 2, 'out_features_0': 6, 'out_features_1': 5, 'learning_rate': 0.277}. Best is trial 2 with value: 0.6833625862065463.\u001b[0m\n",
            "\u001b[32m[I 2021-04-04 13:57:26,167]\u001b[0m Trial 10 finished with value: 0.704184923006892 and parameters: {'num_layers': 2, 'out_features_0': 2, 'out_features_1': 9, 'learning_rate': 0.257}. Best is trial 2 with value: 0.6833625862065463.\u001b[0m\n",
            "\u001b[33m[W 2021-04-04 13:58:16,050]\u001b[0m Trial 11 failed, because the objective function returned nan.\u001b[0m\n",
            "\u001b[33m[W 2021-04-04 13:59:05,750]\u001b[0m Trial 12 failed, because the objective function returned nan.\u001b[0m\n",
            "\u001b[32m[I 2021-04-04 13:59:55,742]\u001b[0m Trial 13 finished with value: 0.6918495047207925 and parameters: {'num_layers': 3, 'out_features_0': 8, 'out_features_1': 4, 'out_features_2': 10, 'learning_rate': 0.081}. Best is trial 2 with value: 0.6833625862065463.\u001b[0m\n",
            "\u001b[32m[I 2021-04-04 14:00:33,034]\u001b[0m Trial 14 finished with value: 0.6951853413285857 and parameters: {'num_layers': 1, 'out_features_0': 4, 'learning_rate': 0.113}. Best is trial 2 with value: 0.6833625862065463.\u001b[0m\n",
            "\u001b[32m[I 2021-04-04 14:01:22,886]\u001b[0m Trial 15 finished with value: 0.6997367548480614 and parameters: {'num_layers': 3, 'out_features_0': 10, 'out_features_1': 8, 'out_features_2': 8, 'learning_rate': 0.401}. Best is trial 2 with value: 0.6833625862065463.\u001b[0m\n",
            "\u001b[32m[I 2021-04-04 14:02:00,149]\u001b[0m Trial 16 finished with value: 0.7009666943832414 and parameters: {'num_layers': 1, 'out_features_0': 2, 'learning_rate': 0.14500000000000002}. Best is trial 2 with value: 0.6833625862065463.\u001b[0m\n",
            "\u001b[32m[I 2021-04-04 14:02:50,014]\u001b[0m Trial 17 finished with value: 0.6996577365192239 and parameters: {'num_layers': 3, 'out_features_0': 4, 'out_features_1': 2, 'out_features_2': 2, 'learning_rate': 0.313}. Best is trial 2 with value: 0.6833625862065463.\u001b[0m\n",
            "\u001b[32m[I 2021-04-04 14:03:39,578]\u001b[0m Trial 18 finished with value: 0.6848414064865195 and parameters: {'num_layers': 3, 'out_features_0': 3, 'out_features_1': 10, 'out_features_2': 6, 'learning_rate': 0.493}. Best is trial 2 with value: 0.6833625862065463.\u001b[0m\n",
            "\u001b[32m[I 2021-04-04 14:04:16,603]\u001b[0m Trial 19 finished with value: 0.6923896884208175 and parameters: {'num_layers': 1, 'out_features_0': 3, 'learning_rate': 0.171}. Best is trial 2 with value: 0.6833625862065463.\u001b[0m\n",
            "\u001b[32m[I 2021-04-04 14:04:59,771]\u001b[0m Trial 20 finished with value: 0.69625552846062 and parameters: {'num_layers': 2, 'out_features_0': 5, 'out_features_1': 4, 'learning_rate': 0.381}. Best is trial 2 with value: 0.6833625862065463.\u001b[0m\n",
            "\u001b[32m[I 2021-04-04 14:05:36,775]\u001b[0m Trial 21 finished with value: 0.6923607590380731 and parameters: {'num_layers': 1, 'out_features_0': 2, 'learning_rate': 0.313}. Best is trial 2 with value: 0.6833625862065463.\u001b[0m\n",
            "\u001b[32m[I 2021-04-04 14:06:26,803]\u001b[0m Trial 22 finished with value: 0.6882168151462297 and parameters: {'num_layers': 3, 'out_features_0': 5, 'out_features_1': 6, 'out_features_2': 6, 'learning_rate': 0.435}. Best is trial 2 with value: 0.6833625862065463.\u001b[0m\n",
            "\u001b[32m[I 2021-04-04 14:07:09,973]\u001b[0m Trial 23 finished with value: 0.6814703767899334 and parameters: {'num_layers': 2, 'out_features_0': 5, 'out_features_1': 4, 'learning_rate': 0.191}. Best is trial 23 with value: 0.6814703767899334.\u001b[0m\n",
            "\u001b[32m[I 2021-04-04 14:07:53,071]\u001b[0m Trial 24 finished with value: 0.6963657203180883 and parameters: {'num_layers': 2, 'out_features_0': 5, 'out_features_1': 4, 'learning_rate': 0.179}. Best is trial 23 with value: 0.6814703767899334.\u001b[0m\n",
            "\u001b[32m[I 2021-04-04 14:08:36,221]\u001b[0m Trial 25 finished with value: 0.6917892040041215 and parameters: {'num_layers': 2, 'out_features_0': 4, 'out_features_1': 3, 'learning_rate': 0.085}. Best is trial 23 with value: 0.6814703767899334.\u001b[0m\n",
            "\u001b[32m[I 2021-04-04 14:09:25,408]\u001b[0m Trial 26 finished with value: 0.6793907373491307 and parameters: {'num_layers': 3, 'out_features_0': 3, 'out_features_1': 5, 'out_features_2': 9, 'learning_rate': 0.20500000000000002}. Best is trial 26 with value: 0.6793907373491307.\u001b[0m\n",
            "\u001b[32m[I 2021-04-04 14:10:14,767]\u001b[0m Trial 27 finished with value: 0.6822737718424063 and parameters: {'num_layers': 3, 'out_features_0': 5, 'out_features_1': 5, 'out_features_2': 9, 'learning_rate': 0.203}. Best is trial 26 with value: 0.6793907373491307.\u001b[0m\n",
            "\u001b[32m[I 2021-04-04 14:11:04,234]\u001b[0m Trial 28 finished with value: 0.690702415855717 and parameters: {'num_layers': 3, 'out_features_0': 8, 'out_features_1': 5, 'out_features_2': 10, 'learning_rate': 0.219}. Best is trial 26 with value: 0.6793907373491307.\u001b[0m\n",
            "\u001b[32m[I 2021-04-04 14:11:53,674]\u001b[0m Trial 29 finished with value: 0.7010601602620363 and parameters: {'num_layers': 3, 'out_features_0': 5, 'out_features_1': 5, 'out_features_2': 8, 'learning_rate': 0.165}. Best is trial 26 with value: 0.6793907373491307.\u001b[0m\n",
            "\u001b[33m[W 2021-04-04 14:12:43,382]\u001b[0m Trial 30 failed, because the objective function returned nan.\u001b[0m\n",
            "\u001b[32m[I 2021-04-04 14:13:32,948]\u001b[0m Trial 31 finished with value: 0.7064524496760811 and parameters: {'num_layers': 3, 'out_features_0': 7, 'out_features_1': 7, 'out_features_2': 8, 'learning_rate': 0.20700000000000002}. Best is trial 26 with value: 0.6793907373491307.\u001b[0m\n",
            "\u001b[33m[W 2021-04-04 14:14:22,906]\u001b[0m Trial 32 failed, because the objective function returned nan.\u001b[0m\n",
            "\u001b[32m[I 2021-04-04 14:15:12,708]\u001b[0m Trial 33 finished with value: 0.6921283129427568 and parameters: {'num_layers': 3, 'out_features_0': 7, 'out_features_1': 5, 'out_features_2': 9, 'learning_rate': 0.271}. Best is trial 26 with value: 0.6793907373491307.\u001b[0m\n",
            "\u001b[32m[I 2021-04-04 14:15:56,469]\u001b[0m Trial 34 finished with value: 0.69498419124335 and parameters: {'num_layers': 2, 'out_features_0': 5, 'out_features_1': 3, 'learning_rate': 0.035}. Best is trial 26 with value: 0.6793907373491307.\u001b[0m\n",
            "\u001b[33m[W 2021-04-04 14:16:46,437]\u001b[0m Trial 35 failed, because the objective function returned nan.\u001b[0m\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UcAaIArJlueF"
      },
      "source": [
        "best_model = study.best_trial.user_attrs[\"model\"]\n",
        "validate(best_model, output=True)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}