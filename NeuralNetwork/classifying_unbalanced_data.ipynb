{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of 05 - tasakaalustamata andmetega klassifitseerimine - ÃœLESANNE",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/envomp/2020-Foundations-of-Artificial-Intelligence-and-Machine-Learning/blob/master/NeuralNetwork/classifying_unbalanced_data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "13IL9UJnKtno"
      },
      "source": [
        "# Unbalanced classification example\n",
        "\n",
        "Loads CSV files from the python list so that each file represents a separate class. Then tries to find the best network configuration and hyperparameters for the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bvFOy9S8KkK4"
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "!pip install optuna\n",
        "import optuna\n",
        "from torch.utils.data import TensorDataset, ConcatDataset, DataLoader\n",
        "import math\n",
        "import copy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ofIR8RrkuPQ1"
      },
      "source": [
        "# download and see what is inside\n",
        "# !wget http://linuxator.com/data/mlaine/data_class1_s.csv\n",
        "# !wget http://linuxator.com/data/mlaine/data_class2_d.csv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G3wSQ-9e6RWU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6697d599-698b-4e6a-b422-286b9fdeaa23"
      },
      "source": [
        "# this is to allow computation on GPU. To use this, enable under Runtime -> Change Runtime type\n",
        "# GPU should be already enabled on most cases when using this sheet as a templete\n",
        "# NB! Be sure to Restart runtime after this or you may get some very odd errors!\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(\"Using\", device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ym59nUirb0e"
      },
      "source": [
        "# this method accepts the list of CSV files\n",
        "# first class (index 0) should be underrepresented class\n",
        "# it will be oversampled 50 times\n",
        "def build_dataset(csv_list, is_big):\n",
        "  class_num = 0\n",
        "  train_datasets = []\n",
        "  val_datasets = []\n",
        "  weights = []\n",
        "  # each file contains a single class\n",
        "  for csv_file in csv_list:\n",
        "    df = pd.read_csv(csv_file)\n",
        "    size = len(df)\n",
        "\n",
        "    if not is_big:\n",
        "      df = df[0:min(len(df), 4000)]\n",
        "    x = torch.from_numpy(df.iloc[:,1:].values.astype(np.float64)).float()\n",
        "\n",
        "    # split train/val  80%/20% in each CSV file\n",
        "    train_cnt = math.floor(x.shape[0] * 0.8)\n",
        "    val_cnt = x.shape[0] - train_cnt\n",
        "\n",
        "    # oversample class 0 which is undersampled\n",
        "    if class_num == 0:\n",
        "      # split 80/20\n",
        "      train_x, val_x = torch.utils.data.random_split(x, [train_cnt, val_cnt])\n",
        "      # repeat class 0 50 times\n",
        "      #print('x', train_x.__len__(), 'val', val_x.__len__())\n",
        "      # we oversample after splitting to avoid having same example both in\n",
        "      # training and validation set\n",
        "      if is_big:\n",
        "        train_x = x[train_x.indices].repeat([50,1])\n",
        "        val_x = x[val_x.indices].repeat([50,1])\n",
        "      else:\n",
        "        train_x = x[train_x.indices].repeat([8,1])\n",
        "        val_x = x[val_x.indices].repeat([8,1])\n",
        "      #print('x', train_x.__len__(), 'val', val_x.__len__())\n",
        "      # generate ground truth tensors\n",
        "      train_y = torch.empty((train_x.shape[0]), dtype=torch.long).fill_(class_num)\n",
        "      val_y = torch.empty((val_x.shape[0]), dtype=torch.long).fill_(class_num)\n",
        "      # compose datasets\n",
        "      train_dataset = TensorDataset(train_x, train_y)\n",
        "      val_dataset = TensorDataset(val_x, val_y)\n",
        "    # keep every other class as they are\n",
        "    else:\n",
        "      # ground truth generation, just use class number\n",
        "      y = torch.empty((x.shape[0]), dtype=torch.long).fill_(class_num)\n",
        "      dataset = TensorDataset(x, y)\n",
        "      # this is how you normally split a dataset\n",
        "      train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_cnt, val_cnt])\n",
        "    \n",
        "    train_datasets.append(train_dataset)\n",
        "    val_datasets.append(val_dataset)\n",
        "\n",
        "    # save num of rows to calculate weights for loss fn\n",
        "    weights.append(train_dataset.__len__())\n",
        "    class_num += 1\n",
        "  \n",
        "  num_classes = class_num\n",
        "  celoss_weights = 1.0 - (torch.tensor(weights).float() / sum(weights))\n",
        "  return ConcatDataset(train_datasets), ConcatDataset(val_datasets), num_classes, celoss_weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2K8Pdo_PIp1P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c1dbaec3-34bf-4548-99b2-cd0293773403"
      },
      "source": [
        "train_dataset, val_dataset, num_classes, celoss_weights = build_dataset(['data_class1_s.csv', 'data_class2_d.csv'], False)\n",
        "print(train_dataset.__len__(), val_dataset.__len__())\n",
        "print(celoss_weights)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "6688 1672\n",
            "tensor([0.4785, 0.5215])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nBSSWGGRK7gw"
      },
      "source": [
        "class ClassificationNetwork(nn.Module):\n",
        "\n",
        "  def __init__(self, num_classes, trial):\n",
        "    super().__init__()\n",
        "    self.num_classes = num_classes\n",
        "    num_layers = trial.suggest_int(\"num_layers\", 1, 3)\n",
        "    in_features = 8\n",
        "    layers = []\n",
        "\n",
        "    for layer_num in range(num_layers):\n",
        "      out_features = trial.suggest_int(\"out_features_{}\".format(layer_num), 2, 10)\n",
        "      layer = nn.Linear(in_features=in_features, out_features=out_features, bias=True)\n",
        "      nn.init.xavier_uniform_(layer.weight)\n",
        "      nn.init.zeros_(layer.bias)\n",
        "      layers.append(layer)\n",
        "      layers.append(nn.Sigmoid())\n",
        "      in_features = out_features\n",
        "      \n",
        "    layers.append(nn.Linear(in_features=in_features, out_features=num_classes))\n",
        "    self.model = nn.Sequential(*layers)\n",
        "\n",
        "  def forward(self, data):\n",
        "    return self.model(data)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tfreiJOhNWTB"
      },
      "source": [
        "def objective(trial): \n",
        "  model = ClassificationNetwork(num_classes, trial).to(device)\n",
        "  \n",
        "  batch_size = 1 # bigger batches don't work\n",
        "  dl = DataLoader(train_dataset,batch_size=batch_size,shuffle=True)\n",
        "\n",
        "  learning_rate = trial.suggest_discrete_uniform(\"learning_rate\", 0.001, 0.499, 0.002)\n",
        "  optimiser = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "  loss = nn.CrossEntropyLoss(celoss_weights).to(device)\n",
        "\n",
        "  epochs = 5\n",
        "  for i in range (0, epochs):\n",
        "    model.train()\n",
        "\n",
        "    epoch_loss = 0.0\n",
        "    for x, y in dl:\n",
        "      x, y = x.to(device), y.to(device)\n",
        "      optimiser.zero_grad()\n",
        "      prediction = model(x)\n",
        "      iter_loss = loss(prediction, y)\n",
        "      epoch_loss += iter_loss\n",
        "      iter_loss.backward()\n",
        "      optimiser.step()\n",
        "\n",
        "    epoch_loss = epoch_loss/dl.__len__()\n",
        "    # print(\"Epoch loss on \", i, \":\", epoch_loss)\n",
        "    trial.set_user_attr(\"model\", model)    \n",
        "    val_loss = validate(model)\n",
        "    trial.report(val_loss, i)\n",
        "    if trial.should_prune():\n",
        "      raise optuna.TrialPruned()\n",
        "\n",
        "  return val_loss\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VvjZ_PkoKTGQ"
      },
      "source": [
        "# Let's write the validation code!\n",
        "\n",
        "1. Put model into evaluation mode\n",
        "2. Use model to predict on validation data\n",
        "3. Evaluate results using loss function\n",
        "\n",
        "Evaluation\n",
        "\n",
        "1. Does not update model parameters\n",
        "2. Hence no need to compute gradients"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iVtHJPNHj4W1"
      },
      "source": [
        "def validate(model, output=False):\n",
        "  model.eval()\n",
        "  total_loss = 0.0\n",
        "  loss = nn.CrossEntropyLoss(celoss_weights.to(device))\n",
        "\n",
        "  dl = DataLoader(val_dataset, batch_size=1)\n",
        "  ok = [0,0]\n",
        "  cls_cnt = [0,0]\n",
        "\n",
        "  with torch.no_grad(): # no gradient calculation inside this block\n",
        "    for data in dl:\n",
        "      x, y = data[0].to(device), data[1].to(device)\n",
        "      prediction = model(x)\n",
        "      \n",
        "      iter_loss = loss(prediction, y)\n",
        "\n",
        "      if output is True:\n",
        "        cls_cnt[y.item()] += 1\n",
        "        norm_pred = torch.nn.functional.softmax(prediction, dim=1)\n",
        "        if norm_pred[0][y.item()] > 0.5:\n",
        "          ok[y.item()] += 1\n",
        "      total_loss += iter_loss.item()\n",
        "\n",
        "  total_loss = total_loss/val_dataset.__len__()\n",
        "  if output is True:\n",
        "    print(\"Mean loss:\", total_loss, \"total\", cls_cnt, \"correct\", ok)\n",
        "\n",
        "  return total_loss\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WTqpkfL_3SKI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "973f6ddc-e062-4566-90ca-40e61ebd8acb"
      },
      "source": [
        "study = optuna.create_study(direction=\"minimize\")\n",
        "study.optimize(objective, n_trials=10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2021-04-04 22:05:49,288]\u001b[0m A new study created in memory with name: no-name-4e48b1fc-a430-4adf-8194-37e4a5c06a1e\u001b[0m\n",
            "\u001b[32m[I 2021-04-04 22:10:17,610]\u001b[0m Trial 0 finished with value: 0.7731490429206095 and parameters: {'num_layers': 1, 'out_features_0': 5, 'learning_rate': 0.447}. Best is trial 0 with value: 0.7731490429206095.\u001b[0m\n",
            "\u001b[32m[I 2021-04-04 22:15:40,234]\u001b[0m Trial 1 finished with value: 0.6748915778337348 and parameters: {'num_layers': 2, 'out_features_0': 8, 'out_features_1': 5, 'learning_rate': 0.027000000000000003}. Best is trial 1 with value: 0.6748915778337348.\u001b[0m\n",
            "\u001b[32m[I 2021-04-04 22:20:07,782]\u001b[0m Trial 2 finished with value: 0.6837587257760975 and parameters: {'num_layers': 1, 'out_features_0': 5, 'learning_rate': 0.069}. Best is trial 1 with value: 0.6748915778337348.\u001b[0m\n",
            "\u001b[32m[I 2021-04-04 22:26:20,425]\u001b[0m Trial 3 finished with value: 0.7016811350211475 and parameters: {'num_layers': 3, 'out_features_0': 8, 'out_features_1': 4, 'out_features_2': 5, 'learning_rate': 0.315}. Best is trial 1 with value: 0.6748915778337348.\u001b[0m\n",
            "\u001b[32m[I 2021-04-04 22:30:47,567]\u001b[0m Trial 4 finished with value: 0.6733963263047652 and parameters: {'num_layers': 1, 'out_features_0': 4, 'learning_rate': 0.265}. Best is trial 4 with value: 0.6733963263047652.\u001b[0m\n",
            "\u001b[32m[I 2021-04-04 22:35:14,726]\u001b[0m Trial 5 finished with value: 0.6788490044416217 and parameters: {'num_layers': 1, 'out_features_0': 7, 'learning_rate': 0.17300000000000001}. Best is trial 4 with value: 0.6733963263047652.\u001b[0m\n",
            "\u001b[32m[I 2021-04-04 22:39:41,477]\u001b[0m Trial 6 finished with value: 0.6744245398630581 and parameters: {'num_layers': 1, 'out_features_0': 10, 'learning_rate': 0.007}. Best is trial 4 with value: 0.6733963263047652.\u001b[0m\n",
            "\u001b[32m[I 2021-04-04 22:40:45,319]\u001b[0m Trial 7 pruned. \u001b[0m\n",
            "\u001b[32m[I 2021-04-04 22:43:56,911]\u001b[0m Trial 8 pruned. \u001b[0m\n",
            "\u001b[32m[I 2021-04-04 22:49:18,037]\u001b[0m Trial 9 finished with value: 0.6702573117621278 and parameters: {'num_layers': 2, 'out_features_0': 6, 'out_features_1': 4, 'learning_rate': 0.055}. Best is trial 9 with value: 0.6702573117621278.\u001b[0m\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UcAaIArJlueF",
        "outputId": "696ef5fc-c036-4804-cd5c-71e16a0a44c6"
      },
      "source": [
        "best_model = study.best_trial.user_attrs[\"model\"]\n",
        "validate(best_model, output=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mean loss: 0.6702573117621278 total [5450, 8485] correct [0, 8485]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6702573117621278"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 157
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4pLxygE84n22",
        "outputId": "5bc55bb4-c83b-486b-a86e-8721f4c3c810"
      },
      "source": [
        "# now try real data\n",
        "\n",
        "train_dataset, val_dataset, num_classes, celoss_weights = build_dataset(['data_class1_s.csv', 'data_class2_d.csv'], True)\n",
        "print(train_dataset.__len__(), val_dataset.__len__())\n",
        "print(celoss_weights)\n",
        "\n",
        "validate(best_model, output=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "55739 13935\n",
            "tensor([0.6089, 0.3911])\n",
            "Mean loss: 0.6702573117621278 total [5450, 8485] correct [0, 8485]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6702573117621278"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 158
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iBYsPjYgAhsa"
      },
      "source": [
        "Since 98.737% of real data belongs into same class, then we can just always predict that class and our average success rate would be that. \n",
        "\n",
        "There are only barely over 500 instances of one class compared to over 42000 instances of other class.\n",
        "\n",
        "After spending 13+ hours on this problem I must admit, that this problem has beaten me. I didn't find a good way of solving this problem using machine learning.\n",
        "\n",
        "Also tried SimpleKMeans, RandomForest and RandomTree classifier using Weka Explorer. \n",
        "\n",
        "Out of them next randomTree gave good results with next modifications:\n",
        "- both CSV files were merged into one.\n",
        "- columns were named with numbers 1 to 9.\n",
        "- new column \"res\" was added which indicated the result class. (0 or 1)\n",
        "- first column was removed as it held no significant information.\n",
        "\n",
        "Command ran: `RandomTree -K 0 -M 1.0 -V 0.001 -S 1`\n",
        "\n",
        "```\n",
        "\n",
        "=== Run information ===\n",
        "\n",
        "Scheme:       weka.classifiers.trees.RandomTree -K 0 -M 1.0 -V 0.001 -S 1\n",
        "Relation:     data_class2_d-weka.filters.unsupervised.attribute.Remove-R1\n",
        "Instances:    42971\n",
        "Attributes:   9\n",
        "              2\n",
        "              3\n",
        "              4\n",
        "              5\n",
        "              6\n",
        "              7\n",
        "              8\n",
        "              9\n",
        "              res\n",
        "Test mode:    evaluate on training data\n",
        "\n",
        "=== Classifier model (full training set) ===\n",
        "\n",
        "\n",
        "RandomTree\n",
        "==========\n",
        "\n",
        "5 < 1359990659\n",
        "|   3 < 3.5\n",
        "|   |   9 < 0.45 : 0 (38376/0)\n",
        "|   |   9 >= 0.45\n",
        "|   |   |   7 < 5.11 : 1 (2/0)\n",
        "|   |   |   7 >= 5.11\n",
        "|   |   |   |   2 < 6392.32\n",
        "|   |   |   |   |   3 < 1.5\n",
        "|   |   |   |   |   |   6 < 1400333549.5 : 1 (4/0)\n",
        "|   |   |   |   |   |   6 >= 1400333549.5 : 0 (1877/0)\n",
        "|   |   |   |   |   3 >= 1.5 : 0 (462/0)\n",
        "|   |   |   |   2 >= 6392.32 : 1 (1/0)\n",
        "|   3 >= 3.5\n",
        "|   |   7 < 1021.01\n",
        "|   |   |   2 < 170.33\n",
        "|   |   |   |   7 < 809.39\n",
        "|   |   |   |   |   8 < 1396.91\n",
        "|   |   |   |   |   |   8 < 1065.57\n",
        "|   |   |   |   |   |   |   8 < 0.02\n",
        "|   |   |   |   |   |   |   |   8 < 0.01\n",
        "|   |   |   |   |   |   |   |   |   5 < 1305287015\n",
        "|   |   |   |   |   |   |   |   |   |   6 < 1398804747 : 1 (1/0)\n",
        "|   |   |   |   |   |   |   |   |   |   6 >= 1398804747 : 0 (38/0)\n",
        "|   |   |   |   |   |   |   |   |   5 >= 1305287015 : 0 (325/0)\n",
        "|   |   |   |   |   |   |   |   8 >= 0.01 : 1 (1/0)\n",
        "|   |   |   |   |   |   |   8 >= 0.02 : 0 (1099/0)\n",
        "|   |   |   |   |   |   8 >= 1065.57\n",
        "|   |   |   |   |   |   |   3 < 36.5 : 0 (7/0)\n",
        "|   |   |   |   |   |   |   3 >= 36.5 : 1 (1/0)\n",
        "|   |   |   |   |   8 >= 1396.91\n",
        "|   |   |   |   |   |   2 < 38.16 : 1 (1/0)\n",
        "|   |   |   |   |   |   2 >= 38.16\n",
        "|   |   |   |   |   |   |   4 < 0.5 : 0 (1/0)\n",
        "|   |   |   |   |   |   |   4 >= 0.5 : 0.5 (6/0.25)\n",
        "|   |   |   |   7 >= 809.39\n",
        "|   |   |   |   |   9 < 0.29 : 0 (4/0)\n",
        "|   |   |   |   |   9 >= 0.29\n",
        "|   |   |   |   |   |   4 < 30 : 0 (1/0)\n",
        "|   |   |   |   |   |   4 >= 30 : 1 (2/0)\n",
        "|   |   |   2 >= 170.33\n",
        "|   |   |   |   6 < 1428003360\n",
        "|   |   |   |   |   5 < 1347153618\n",
        "|   |   |   |   |   |   6 < 1411012367 : 0 (2/0)\n",
        "|   |   |   |   |   |   6 >= 1411012367\n",
        "|   |   |   |   |   |   |   2 < 206.49 : 0.5 (4/0.25)\n",
        "|   |   |   |   |   |   |   2 >= 206.49\n",
        "|   |   |   |   |   |   |   |   2 < 227.78 : 0 (1/0)\n",
        "|   |   |   |   |   |   |   |   2 >= 227.78\n",
        "|   |   |   |   |   |   |   |   |   3 < 8.5 : 0 (1/0)\n",
        "|   |   |   |   |   |   |   |   |   3 >= 8.5 : 0.5 (4/0.25)\n",
        "|   |   |   |   |   5 >= 1347153618 : 1 (1/0)\n",
        "|   |   |   |   6 >= 1428003360\n",
        "|   |   |   |   |   8 < 1500 : 0 (66/0)\n",
        "|   |   |   |   |   8 >= 1500 : 0.5 (2/0.25)\n",
        "|   |   7 >= 1021.01\n",
        "|   |   |   8 < 1460.5 : 0 (62/0)\n",
        "|   |   |   8 >= 1460.5\n",
        "|   |   |   |   6 < 1427998514\n",
        "|   |   |   |   |   5 < 1312184732\n",
        "|   |   |   |   |   |   2 < 380.36\n",
        "|   |   |   |   |   |   |   2 < 366.07\n",
        "|   |   |   |   |   |   |   |   8 < 2582.97 : 0 (3/0)\n",
        "|   |   |   |   |   |   |   |   8 >= 2582.97 : 0.5 (2/0.25)\n",
        "|   |   |   |   |   |   |   2 >= 366.07 : 0.5 (2/0.25)\n",
        "|   |   |   |   |   |   2 >= 380.36 : 0 (9/0)\n",
        "|   |   |   |   |   5 >= 1312184732\n",
        "|   |   |   |   |   |   5 < 1334063058 : 0.5 (6/0.25)\n",
        "|   |   |   |   |   |   5 >= 1334063058\n",
        "|   |   |   |   |   |   |   5 < 1338578613 : 0 (1/0)\n",
        "|   |   |   |   |   |   |   5 >= 1338578613 : 0.5 (2/0.25)\n",
        "|   |   |   |   6 >= 1427998514\n",
        "|   |   |   |   |   4 < 52.5\n",
        "|   |   |   |   |   |   2 < 1635.85\n",
        "|   |   |   |   |   |   |   3 < 4.5\n",
        "|   |   |   |   |   |   |   |   7 < 1884.78\n",
        "|   |   |   |   |   |   |   |   |   5 < 1296852383\n",
        "|   |   |   |   |   |   |   |   |   |   5 < 1287477484.5 : 0.5 (2/0.25)\n",
        "|   |   |   |   |   |   |   |   |   |   5 >= 1287477484.5 : 0 (1/0)\n",
        "|   |   |   |   |   |   |   |   |   5 >= 1296852383 : 0.5 (12/0.25)\n",
        "|   |   |   |   |   |   |   |   7 >= 1884.78 : 0.5 (18/0.25)\n",
        "|   |   |   |   |   |   |   3 >= 4.5\n",
        "|   |   |   |   |   |   |   |   8 < 3205.35\n",
        "|   |   |   |   |   |   |   |   |   5 < 1322418959.5\n",
        "|   |   |   |   |   |   |   |   |   |   8 < 1669.21 : 0.5 (4/0.25)\n",
        "|   |   |   |   |   |   |   |   |   |   8 >= 1669.21\n",
        "|   |   |   |   |   |   |   |   |   |   |   3 < 5.5 : 0 (4/0)\n",
        "|   |   |   |   |   |   |   |   |   |   |   3 >= 5.5\n",
        "|   |   |   |   |   |   |   |   |   |   |   |   5 < 1293890987.5 : 0.5 (4/0.25)\n",
        "|   |   |   |   |   |   |   |   |   |   |   |   5 >= 1293890987.5 : 0 (2/0)\n",
        "|   |   |   |   |   |   |   |   |   5 >= 1322418959.5\n",
        "|   |   |   |   |   |   |   |   |   |   8 < 2000 : 0.5 (8/0.25)\n",
        "|   |   |   |   |   |   |   |   |   |   8 >= 2000\n",
        "|   |   |   |   |   |   |   |   |   |   |   2 < 333.33 : 0 (1/0)\n",
        "|   |   |   |   |   |   |   |   |   |   |   2 >= 333.33 : 0.5 (2/0.25)\n",
        "|   |   |   |   |   |   |   |   8 >= 3205.35\n",
        "|   |   |   |   |   |   |   |   |   3 < 22.5\n",
        "|   |   |   |   |   |   |   |   |   |   3 < 19\n",
        "|   |   |   |   |   |   |   |   |   |   |   2 < 428.06\n",
        "|   |   |   |   |   |   |   |   |   |   |   |   2 < 408.89\n",
        "|   |   |   |   |   |   |   |   |   |   |   |   |   8 < 3359 : 0.5 (6/0.25)\n",
        "|   |   |   |   |   |   |   |   |   |   |   |   |   8 >= 3359\n",
        "|   |   |   |   |   |   |   |   |   |   |   |   |   |   5 < 1301591468.5 : 0.5 (2/0.25)\n",
        "|   |   |   |   |   |   |   |   |   |   |   |   |   |   5 >= 1301591468.5 : 0 (1/0)\n",
        "|   |   |   |   |   |   |   |   |   |   |   |   2 >= 408.89 : 0 (1/0)\n",
        "|   |   |   |   |   |   |   |   |   |   |   2 >= 428.06\n",
        "|   |   |   |   |   |   |   |   |   |   |   |   5 < 1297253869.5\n",
        "|   |   |   |   |   |   |   |   |   |   |   |   |   3 < 14.5 : 0.5 (6/0.25)\n",
        "|   |   |   |   |   |   |   |   |   |   |   |   |   3 >= 14.5 : 0 (2/0)\n",
        "|   |   |   |   |   |   |   |   |   |   |   |   5 >= 1297253869.5\n",
        "|   |   |   |   |   |   |   |   |   |   |   |   |   2 < 650.11 : 0.5 (30/0.25)\n",
        "|   |   |   |   |   |   |   |   |   |   |   |   |   2 >= 650.11\n",
        "|   |   |   |   |   |   |   |   |   |   |   |   |   |   2 < 683.33 : 0 (1/0)\n",
        "|   |   |   |   |   |   |   |   |   |   |   |   |   |   2 >= 683.33 : 0.5 (6/0.25)\n",
        "|   |   |   |   |   |   |   |   |   |   3 >= 19 : 0 (1/0)\n",
        "|   |   |   |   |   |   |   |   |   3 >= 22.5 : 0.5 (16/0.25)\n",
        "|   |   |   |   |   |   2 >= 1635.85\n",
        "|   |   |   |   |   |   |   2 < 6373.01 : 0 (2/0)\n",
        "|   |   |   |   |   |   |   2 >= 6373.01 : 0.5 (2/0.25)\n",
        "|   |   |   |   |   4 >= 52.5\n",
        "|   |   |   |   |   |   2 < 365.26 : 0.5 (2/0.25)\n",
        "|   |   |   |   |   |   2 >= 365.26 : 1 (1/0)\n",
        "5 >= 1359990659 : 1 (457/0)\n",
        "\n",
        "Size of the tree : 123\n",
        "\n",
        "Time taken to build model: 0.05 seconds\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RZ28YzhDF5Gb",
        "outputId": "2d28579a-d26b-421a-9294-dd47d211ebe7"
      },
      "source": [
        "import csv\n",
        "\n",
        "with open('res.csv', 'r') as file:\n",
        "    reader = csv.reader(file, delimiter=',')\n",
        "\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for i, a, p, e in reader:\n",
        "        if i.isdigit():\n",
        "            total += 1\n",
        "\n",
        "            if p == '?' or float(a) < 0.5 and p == \"0\": # on failure default to class 0\n",
        "                correct += 1\n",
        "            if float(a) >= 0.5 and p == \"1\":\n",
        "                correct += 1\n",
        "\n",
        "    print(\"Correct guesses out of all: \" + str(correct) + \" / \" + str(total))\n",
        "    print(\"Percentually it is: \" + str(round(correct / total * 100, 2)) + \"%\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Correct guesses out of all: 42823 / 42971\n",
            "Percentually it is: 99.66%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T_OHe51IHezz"
      },
      "source": [
        "All-in-all it was a good exercise but I was unsuccessful in choosing and implementing a high performing neural network."
      ]
    }
  ]
}